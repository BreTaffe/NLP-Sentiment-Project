{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "RH2UE_gHO0Z8",
        "outputId": "baebb418-028d-4da0-ba94-a5f46fc5984f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-773515840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0muser_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a product review: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_review\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'q'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Sentiment Analysis - Customer Reviews\n",
        "# COP4023 Programming Languages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Deep Learning Libraries\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Sklearn Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# ============================================================================\n",
        "# DATA WRANGLING\n",
        "# ============================================================================\n",
        "\n",
        "# 1. Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('customer_reviewers.tsv', sep='\\t')\n",
        "print(\"Dataset loaded successfully!\\n\")\n",
        "\n",
        "# 2. Display first 10 rows\n",
        "print(\"=\" * 80)\n",
        "print(\"FIRST 10 ROWS OF THE DATASET\")\n",
        "print(\"=\" * 80)\n",
        "print(df.head(10))\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. Check for missing values\n",
        "print(\"=\" * 80)\n",
        "print(\"CHECKING FOR MISSING VALUES\")\n",
        "print(\"=\" * 80)\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Handle missing values if any\n",
        "df = df.dropna(subset=['verified_reviews', 'feedback'])\n",
        "print(f\"Dataset shape after removing missing values: {df.shape}\\n\")\n",
        "\n",
        "# 4. Text Preprocessing Function - Remove stopwords\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data:\n",
        "    - Convert to lowercase\n",
        "    - Remove special characters and numbers\n",
        "    - Remove stopwords\n",
        "    - Lemmatization\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"=\" * 80)\n",
        "print(\"PREPROCESSING TEXT DATA (Removing stopwords, lemmatization)\")\n",
        "print(\"=\" * 80)\n",
        "df['cleaned_reviews'] = df['verified_reviews'].apply(preprocess_text)\n",
        "print(\"Text preprocessing completed!\\n\")\n",
        "\n",
        "# Display sample of cleaned data\n",
        "print(\"Sample of cleaned reviews:\")\n",
        "print(df[['verified_reviews', 'cleaned_reviews']].head(3))\n",
        "print(\"\\n\")\n",
        "\n",
        "# 5. Generate WordCloud\n",
        "print(\"=\" * 80)\n",
        "print(\"GENERATING WORD CLOUD\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Combine all cleaned reviews\n",
        "all_text = ' '.join(df['cleaned_reviews'])\n",
        "\n",
        "# Create WordCloud\n",
        "wordcloud = WordCloud(width=800, height=400,\n",
        "                      background_color='white',\n",
        "                      colormap='viridis',\n",
        "                      max_words=100).generate(all_text)\n",
        "\n",
        "# Display WordCloud\n",
        "plt.figure(figsize=(15, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Most Frequent Words in Customer Reviews', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print(\"Word Cloud generated successfully!\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DATA ENGINEERING - ENCODING AND TOKENIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Encode target labels (feedback: 0 or 1)\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['feedback'])\n",
        "print(f\"Label encoding completed. Classes: {label_encoder.classes_}\")\n",
        "print(f\"Encoded labels shape: {y.shape}\\n\")\n",
        "\n",
        "# 2. Tokenization - Convert text to sequences\n",
        "MAX_WORDS = 5000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['cleaned_reviews'])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_reviews'])\n",
        "\n",
        "# Pad sequences\n",
        "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "print(f\"Tokenization completed!\")\n",
        "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
        "print(f\"Padded sequences shape: {X.shape}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DESIGN - LSTM\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BUILDING LSTM MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Model configuration\n",
        "EMBEDDING_DIM = 120\n",
        "LSTM_UNITS = 176\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# 1. Embedding Layer\n",
        "model.add(Embedding(input_dim=MAX_WORDS, # Changed from 500 to MAX_WORDS\n",
        "                   output_dim=EMBEDDING_DIM,\n",
        "                   input_length=X.shape[1]))\n",
        "\n",
        "# 2. SpatialDropout1D\n",
        "model.add(SpatialDropout1D(0.3)) # Reduced from 0.4\n",
        "\n",
        "# 3. LSTM Layer\n",
        "model.add(LSTM(LSTM_UNITS, dropout=0.1, recurrent_dropout=0.1)) # Reduced from 0.2\n",
        "\n",
        "# 4. Dense Output Layer\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(\"=\" * 80)\n",
        "model.summary()\n",
        "print(\"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA SPLITTING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SPLITTING DATA - TRAINING/TESTING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. One-hot encode labels\n",
        "y_categorical = to_categorical(y, num_classes=2)\n",
        "print(f\"One-hot encoded labels shape: {y_categorical.shape}\")\n",
        "\n",
        "# 2. Split data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_categorical,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_categorical\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING THE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING THE MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate class weights to handle imbalance\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(np.argmax(y_train, axis=1)),\n",
        "    y=np.argmax(y_train, axis=1)\n",
        ")\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "print(f\"Calculated class weights: {class_weights_dict}\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10, # Increased epochs from 5 to 10\n",
        "    batch_size=32,\n",
        "    verbose='auto',\n",
        "    validation_split=0.1,\n",
        "    class_weight=class_weights_dict  # Apply class weights here\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\\n\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# EVALUATE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATING MODEL ON TEST DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 2. Convert predictions and true labels using argmax\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(f\"Predictions shape: {y_pred_classes.shape}\")\n",
        "print(f\"True labels shape: {y_test_classes.shape}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# PERFORMANCE EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(\n",
        "    y_test_classes,\n",
        "    y_pred_classes,\n",
        "    target_names=['Negative (0)', 'Positive (1)'],\n",
        "    digits=4\n",
        ")\n",
        "\n",
        "print(report)\n",
        "print(\"\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# CLIENT PROGRAM - INTERACTIVE PREDICTION\n",
        "# ============================================================================\n",
        "\n",
        "def predict_sentiment(review_text):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a new review\n",
        "    \"\"\"\n",
        "    # 1. Clean the review (remove stopwords)\n",
        "    cleaned = preprocess_text(review_text)\n",
        "\n",
        "    # 2. Convert to sequence\n",
        "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
        "\n",
        "    # 3. Pad sequence\n",
        "    padded = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "    # 4. Predict\n",
        "    prediction = model.predict(padded, verbose=0)\n",
        "    sentiment_class = np.argmax(prediction, axis=1)[0]\n",
        "    confidence = prediction[0][sentiment_class] * 100\n",
        "\n",
        "    sentiment = \"Positive\" if sentiment_class == 1 else \"Negative\"\n",
        "\n",
        "    return sentiment, confidence\n",
        "\n",
        "# Interactive client program\n",
        "print(\"=\" * 80)\n",
        "print(\"CLIENT PROGRAM - SENTIMENT PREDICTION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nWelcome to the Sentiment Analysis System!\")\n",
        "print(\"Enter a product review to predict its sentiment.\")\n",
        "print(\"Type 'quit' to exit.\\n\")\n",
        "\n",
        "# Example predictions\n",
        "sample_reviews = [\n",
        "    \"This product is absolutely amazing! I love it so much and highly recommend it.\",\n",
        "    \"Terrible quality. Waste of money. Very disappointed with this purchase.\",\n",
        "    \"It's okay, nothing special. Works as expected but could be better.\"\n",
        "]\n",
        "\n",
        "print(\"Demo: Testing with sample reviews:\\n\")\n",
        "for i, review in enumerate(sample_reviews, 1):\n",
        "    sentiment, confidence = predict_sentiment(review)\n",
        "    print(f\"Review {i}: {review[:60]}...\")\n",
        "    print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.2f}%)\")\n",
        "    print(\"-\" * 80)\n",
        "    print()\n",
        "\n",
        "# Interactive mode\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"INTERACTIVE MODE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"You can now enter your own review for sentiment analysis.\")\n",
        "print(\"Type 'quit' to exit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_review = input(\"Enter a product review: \")\n",
        "\n",
        "    if user_review.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"\\nThank you for using the Sentiment Analysis System!\")\n",
        "        break\n",
        "\n",
        "    if user_review.strip():\n",
        "        sentiment, confidence = predict_sentiment(user_review)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Predicted Sentiment: {sentiment}\")\n",
        "        print(f\"Confidence: {confidence:.2f}%\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "    else:\n",
        "        print(\"Please enter a valid review.\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PROGRAM COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d659a836"
      },
      "source": [
        "print(df['feedback'].value_counts())\n",
        "print(df['feedback'].value_counts(normalize=True) * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c015d25c"
      },
      "source": [
        "This output shows the count and percentage of each sentiment class in your `feedback` column. A significant difference in these numbers would confirm a class imbalance. If there is a strong imbalance, we will need to address it to improve the model's ability to learn both classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"customer_reviewers.tsv\", sep=\"\\t\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5qf7652zRx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape        # shows number of rows and columns\n",
        "df.columns      # shows column names\n",
        "df.head(10)     # shows first 10 rows"
      ],
      "metadata": {
        "id": "UFado0XFR_QI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}